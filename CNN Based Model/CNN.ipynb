{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h89Z7cNC9u-x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import precision_recall_curve, auc, r2_score\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_RzgrNgrezA",
    "outputId": "ff23d635-ad6a-4384-e6e2-7c195b92fb57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/vvnf_7lj4fq51msr0tlfkjhc0000gn/T/ipykernel_36221/1428665829.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_data = pd.concat([train_data, train_group])\n",
      "/var/folders/4r/vvnf_7lj4fq51msr0tlfkjhc0000gn/T/ipykernel_36221/1428665829.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_data = pd.concat([test_data, test_group])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"davis.txt\"\n",
    "columns = [\"drug_id\", \"protein_id\", \"SMILES_sequence\", \"amino_acid_sequence\", \"affinity_score\"]\n",
    "data = pd.read_csv(file_path, sep = ' ', names=columns)\n",
    "\n",
    "grouping = data.groupby(\"protein_id\")\n",
    "\n",
    "train_data = pd.DataFrame(columns = data.columns)\n",
    "test_data = pd.DataFrame(columns = data.columns)\n",
    "\n",
    "for protein_id, group in grouping:\n",
    "    train_group, test_group = train_test_split(group, test_size = 0.7, random_state = 36)\n",
    "    train_data = pd.concat([train_data, train_group])\n",
    "    test_data = pd.concat([test_data, test_group])\n",
    "\n",
    "train_data.to_csv(\"Train_Data.csv\", index = False )\n",
    "test_data.to_csv(\"Test_Data.csv\", index = False)\n",
    "\n",
    "unique_proteins = data['protein_id'].unique()\n",
    "train_proteins, test_proteins = train_test_split(unique_proteins, test_size=0.1, random_state=36)\n",
    "\n",
    "train_data_new_proteins = data[data['protein_id'].isin(train_proteins)]\n",
    "test_data_new_proteins = data[data['protein_id'].isin(test_proteins)]\n",
    "\n",
    "train_data_new_proteins.to_csv(\"Train_Data_new_proteins.csv\", index=False)\n",
    "test_data_new_proteins.to_csv(\"Test_Data_new_proteins.csv\", index=False)\n",
    "\n",
    "# No new drugs in test\n",
    "train_data_no_new_drugs, test_data_no_new_drugs = train_test_split(data, test_size=0.2, random_state=36)\n",
    "train_drugs = train_data_no_new_drugs['drug_id'].unique()\n",
    "test_data_no_new_drugs = test_data_no_new_drugs[test_data_no_new_drugs['drug_id'].isin(train_drugs)]\n",
    "\n",
    "train_data_no_new_drugs.to_csv(\"Train_Data_no_new_drugs.csv\", index=False)\n",
    "test_data_no_new_drugs.to_csv(\"Test_Data_no_new_drugs.csv\", index=False)\n",
    "\n",
    "# New drugs in test\n",
    "unique_drugs = data['drug_id'].unique()\n",
    "train_drugs, test_drugs = train_test_split(unique_drugs, test_size=0.1, random_state=36)\n",
    "\n",
    "train_data_new_drugs = data[data['drug_id'].isin(train_drugs)]\n",
    "test_data_new_drugs = data[data['drug_id'].isin(test_drugs)]\n",
    "\n",
    "train_data_new_drugs.to_csv(\"Train_Data_new_drugs.csv\", index=False)\n",
    "test_data_new_drugs.to_csv(\"Test_Data_new_drugs.csv\", index=False)\n",
    "\n",
    "train_data = pd.read_csv(\"Train_Data_new_proteins.csv\")\n",
    "test_data = pd.read_csv(\"Test_Data_new_proteins.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDko_iDV9u-z",
    "outputId": "62a2153b-e137-4caa-c45c-d797e8bde92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES Dictionary: {'4': 1, 'S': 2, 'F': 3, 'C': 4, 'N': 5, 'O': 6, '.': 7, '1': 8, '7': 9, ')': 10, '5': 11, 'I': 12, '2': 13, 'r': 14, '=': 15, 'B': 16, 'l': 17, '8': 18, 'P': 19, '9': 20, '(': 21, '6': 22, '3': 23, '#': 24}\n"
     ]
    }
   ],
   "source": [
    "amino_acid_dict = {\n",
    "    'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5,\n",
    "    'E': 6, 'Q': 7, 'G': 8, 'H': 9, 'I': 10,\n",
    "    'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15,\n",
    "    'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20\n",
    "}\n",
    "\n",
    "def encode_aa_sequence(sequence, aa_dict, max_len = 1000):\n",
    "   encoded_aa_sequence = [aa_dict[aa] for aa in sequence if aa in aa_dict]\n",
    "   encoded_aa_sequence = encoded_aa_sequence[:max_len]\n",
    "   padding = [0] * (max_len - len(encoded_aa_sequence))\n",
    "   return encoded_aa_sequence + padding\n",
    "\n",
    "train_data['encoded_aa_sequence'] = train_data['amino_acid_sequence'].apply(lambda x: encode_aa_sequence(x, amino_acid_dict))\n",
    "test_data['encoded_aa_sequence'] = test_data['amino_acid_sequence'].apply(lambda x: encode_aa_sequence(x, amino_acid_dict))\n",
    "\n",
    "all_smiles_chars = set(\"\".join(data['SMILES_sequence']))\n",
    "smiles_dict = {char: idx + 1 for idx, char in enumerate(all_smiles_chars)}  # Reserve 0 for padding\n",
    "\n",
    "print(\"SMILES Dictionary:\", smiles_dict)\n",
    "\n",
    "all_smiles_chars = set(\"\".join(data['SMILES_sequence']))\n",
    "smiles_dict = {char: idx for idx, char in enumerate(sorted(all_smiles_chars))}\n",
    "\n",
    "def encode_smiles_onehot(smiles, smiles_dict, max_len=150):\n",
    "    encoding = [[0] * len(smiles_dict) for _ in range(max_len)]\n",
    "    for i, char in enumerate(smiles[:max_len]):\n",
    "        if char in smiles_dict:\n",
    "            encoding[i][smiles_dict[char]] = 1\n",
    "    return encoding\n",
    "\n",
    "\n",
    "train_data['encoded_smiles_onehot'] = train_data['SMILES_sequence'].apply(lambda x: encode_smiles_onehot(x, smiles_dict))\n",
    "test_data['encoded_smiles_onehot'] = test_data['SMILES_sequence'].apply(lambda x: encode_smiles_onehot(x, smiles_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z8oDPZow9u-z"
   },
   "outputs": [],
   "source": [
    "class AffinityDataset (Dataset):\n",
    "  def __init__ (self, aa_en_seq, smiles_en_seq, scores):\n",
    "    self.aa_en_seq = aa_en_seq\n",
    "    self.smiles_en_seq = smiles_en_seq\n",
    "    self.scores = scores\n",
    "\n",
    "  def __len__ (self):\n",
    "    return len(self.aa_en_seq)\n",
    "\n",
    "  def __getitem__ (self, idx):\n",
    "    aa_sequence = torch.tensor(self.aa_en_seq[idx], dtype = torch.float)\n",
    "    smiles_sequence = torch.tensor(self.smiles_en_seq[idx], dtype = torch.float)\n",
    "    score = torch.tensor(self.scores[idx], dtype = torch.float)\n",
    "\n",
    "    return (aa_sequence, smiles_sequence), score\n",
    "\n",
    "train_dataset = AffinityDataset(train_data['encoded_aa_sequence'].tolist(),\n",
    "                                  train_data['encoded_smiles_onehot'].tolist(),\n",
    "                                  train_data['affinity_score'].tolist())\n",
    "test_dataset = AffinityDataset(test_data['encoded_aa_sequence'].tolist(),\n",
    "                                 test_data['encoded_smiles_onehot'].tolist(),\n",
    "                                 test_data['affinity_score'].tolist())\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "66t8LHur9u-z"
   },
   "outputs": [],
   "source": [
    "class CNNAffinityPredictor(nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, aa_vocab_size=21, smiles_seq_len=150, aa_seq_len=1000, embedding_dim=32, output_dim=1):\n",
    "        super(CNNAffinityPredictor, self).__init__()\n",
    "\n",
    "        # SMILES branch\n",
    "        self.smiles_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=smiles_vocab_size, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "\n",
    "        # Protein branch\n",
    "        self.aa_embedding = nn.Embedding(num_embeddings=aa_vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.aa_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "\n",
    "        # Regression head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, smiles_input, aa_input):\n",
    "        # SMILES: [batch, seq_len, vocab] → [batch, vocab, seq_len]\n",
    "        smiles_input = smiles_input.permute(0, 2, 1)\n",
    "        smiles_feat = self.smiles_conv(smiles_input).squeeze(-1)\n",
    "\n",
    "        # Proteins: [batch, seq_len] → embed → [batch, embed_dim, seq_len]\n",
    "        aa_embed = self.aa_embedding(aa_input.long()).permute(0, 2, 1)\n",
    "        aa_feat = self.aa_conv(aa_embed).squeeze(-1)\n",
    "\n",
    "        combined = torch.cat((smiles_feat, aa_feat), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "smiles_input_dim = len(train_dataset[0][0][1][0]) * len(train_dataset[0][0][1])  # Update smiles_input_dim\n",
    "aa_input_dim = len(train_dataset[0][0][0])  # Update aa_input_dim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNAffinityPredictor(smiles_vocab_size=len(smiles_dict)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37428106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b425e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8027\n",
      "Epoch 2/10, Loss: 0.5758\n",
      "Epoch 3/10, Loss: 0.5637\n",
      "Epoch 4/10, Loss: 0.5368\n",
      "Epoch 5/10, Loss: 0.5151\n",
      "Epoch 6/10, Loss: 0.4934\n",
      "Epoch 7/10, Loss: 0.4858\n",
      "Epoch 8/10, Loss: 0.4786\n",
      "Epoch 9/10, Loss: 0.4804\n",
      "Epoch 10/10, Loss: 0.4793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for (aa_seq, smiles_seq), labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(smiles_seq, aa_seq).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4eb3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rm2_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    slope = np.sum(y_true * y_pred) / np.sum(y_pred ** 2)\n",
    "    y_pred_origin = slope * y_pred\n",
    "    r0_squared = r2_score(y_true, y_pred_origin)\n",
    "    return r2 * (1 - np.sqrt(np.abs(r2 - r0_squared)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631a3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI: 0.800, MSE: 0.531, R²: 0.389, R: 0.627, Rm²: 0.364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (aa_seq, smiles_seq), labels in test_loader:\n",
    "        outputs = model(smiles_seq, aa_seq).squeeze()\n",
    "        preds.extend(outputs.tolist())\n",
    "        trues.extend(labels.tolist())\n",
    "\n",
    "ci = concordance_index(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "r2 = r2_score(trues, preds)\n",
    "r = np.corrcoef(trues, preds)[0, 1]\n",
    "rm2 = rm2_score(trues, preds)\n",
    "\n",
    "print(f\"CI: {ci:.3f}, MSE: {mse:.3f}, R²: {r2:.3f}, R: {r:.3f}, Rm²: {rm2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w3sOtK8ADJ1b"
   },
   "outputs": [],
   "source": [
    "def calculate_ci(y_true, y_pred):\n",
    "    return concordance_index(y_true, y_pred)\n",
    "\n",
    "def calculate_auc(y_true, y_pred):\n",
    "    threshold = np.median(y_true)\n",
    "    y_true_binary = (y_true >= threshold).astype(int)\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true_binary, y_pred_binary)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def rm2score(r2_score):\n",
    "   return r2_score * (1 - (abs(r2_score - 0.5) ** 0.5))\n",
    "\n",
    "def calculate_pearson (y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "gv4rf-j6NbP-",
    "outputId": "3a6d33d4-4bee-4d0a-fdba-322b1e445c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on split: No New Proteins in Test\n",
      "Epoch 1, Loss: 1.3436\n",
      "Epoch 2, Loss: 0.6529\n",
      "Epoch 3, Loss: 0.6478\n",
      "Epoch 4, Loss: 0.6464\n",
      "Epoch 5, Loss: 0.6438\n",
      "Epoch 6, Loss: 0.6332\n",
      "Epoch 7, Loss: 0.6403\n",
      "Epoch 8, Loss: 0.6392\n",
      "Epoch 9, Loss: 0.6258\n",
      "Epoch 10, Loss: 0.6482\n",
      "Evaluating on split: No New Proteins in Test\n",
      "Metrics: MSE=0.6586, CI=0.6906, R2=0.1022, AUC-PR=1.0000, RM²=0.0377, Pearson=0.3469\n",
      "Training on split: New Proteins in Test\n",
      "Epoch 1, Loss: 0.7809\n",
      "Epoch 2, Loss: 0.5735\n",
      "Epoch 3, Loss: 0.5682\n",
      "Epoch 4, Loss: 0.5303\n",
      "Epoch 5, Loss: 0.5060\n",
      "Epoch 6, Loss: 0.4914\n",
      "Epoch 7, Loss: 0.4801\n",
      "Epoch 8, Loss: 0.4763\n",
      "Epoch 9, Loss: 0.4616\n",
      "Epoch 10, Loss: 0.4561\n",
      "Evaluating on split: New Proteins in Test\n",
      "Metrics: MSE=0.5251, CI=0.8015, R2=0.3937, AUC-PR=1.0000, RM²=0.2654, Pearson=0.6339\n",
      "Training on split: No New Drugs in Test\n",
      "Epoch 1, Loss: 0.8352\n",
      "Epoch 2, Loss: 0.5741\n",
      "Epoch 3, Loss: 0.5705\n",
      "Epoch 4, Loss: 0.5740\n",
      "Epoch 5, Loss: 0.5611\n",
      "Epoch 6, Loss: 0.5501\n",
      "Epoch 7, Loss: 0.5253\n",
      "Epoch 8, Loss: 0.4966\n",
      "Epoch 9, Loss: 0.4943\n",
      "Epoch 10, Loss: 0.4815\n",
      "Evaluating on split: No New Drugs in Test\n",
      "Metrics: MSE=0.4967, CI=0.8115, R2=0.3840, AUC-PR=1.0000, RM²=0.2532, Pearson=0.6255\n",
      "Training on split: New Drugs in Test\n",
      "Epoch 1, Loss: 0.8461\n",
      "Epoch 2, Loss: 0.6047\n",
      "Epoch 3, Loss: 0.5969\n",
      "Epoch 4, Loss: 0.5892\n",
      "Epoch 5, Loss: 0.5655\n",
      "Epoch 6, Loss: 0.5356\n",
      "Epoch 7, Loss: 0.5219\n",
      "Epoch 8, Loss: 0.5068\n",
      "Epoch 9, Loss: 0.5010\n",
      "Epoch 10, Loss: 0.4976\n",
      "Evaluating on split: New Drugs in Test\n",
      "Metrics: MSE=0.6286, CI=0.4702, R2=-0.2501, AUC-PR=1.0000, RM²=-0.0335, Pearson=-0.0032\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_on_split(train_data_path, split_name):\n",
    "    print(f\"Training on split: {split_name}\")\n",
    "\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data['encoded_aa_sequence'] = train_data['amino_acid_sequence'].apply(lambda x: encode_aa_sequence(x, amino_acid_dict))\n",
    "    train_data['encoded_smiles_onehot'] = train_data['SMILES_sequence'].apply(lambda x: encode_smiles_onehot(x, smiles_dict))\n",
    "    # Load training data\n",
    "    train_dataset = AffinityDataset(train_data['encoded_aa_sequence'].tolist(),\n",
    "                                      train_data['encoded_smiles_onehot'].tolist(),\n",
    "                                      train_data['affinity_score'].tolist())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    smiles_input_dim = len(train_dataset[0][0][1][0]) * len(train_dataset[0][0][1])\n",
    "    aa_input_dim = len(train_dataset[0][0][0])\n",
    "\n",
    "    # Define model, criterion, optimizer\n",
    "    model = CNNAffinityPredictor(\n",
    "    smiles_vocab_size=len(smiles_dict),\n",
    "    aa_vocab_size=21,\n",
    "    smiles_seq_len=150,\n",
    "    aa_seq_len=1000,\n",
    "    embedding_dim=32,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (aa_seq, smiles_seq), labels in train_loader:\n",
    "            aa_seq, smiles_seq, labels = aa_seq.to(device), smiles_seq.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(smiles_seq, aa_seq).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    return model, train_loader\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_on_split(model, test_data_path, split_name):\n",
    "    print(f\"Evaluating on split: {split_name}\")\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data['encoded_aa_sequence'] = test_data['amino_acid_sequence'].apply(lambda x: encode_aa_sequence(x, amino_acid_dict))\n",
    "    test_data['encoded_smiles_onehot'] = test_data['SMILES_sequence'].apply(lambda x: encode_smiles_onehot(x, smiles_dict))\n",
    "    # Load test data\n",
    "    test_dataset = AffinityDataset(\n",
    "        test_data['encoded_aa_sequence'].tolist(),\n",
    "        test_data['encoded_smiles_onehot'].tolist(),\n",
    "        test_data['affinity_score'].tolist()\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, test_loader, nn.MSELoss(), device)\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (aa_seq, smiles_seq), labels in data_loader:\n",
    "            aa_seq, smiles_seq, labels = aa_seq.to(device), smiles_seq.to(device), labels.to(device)\n",
    "            outputs = model(smiles_seq, aa_seq).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    mse = total_loss / len(data_loader)\n",
    "    ci = calculate_ci(np.array(y_true), np.array(y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    auc_pr = calculate_auc(np.array(y_true), np.array(y_pred))\n",
    "    rm2 = rm2score(r2)\n",
    "    pearson = calculate_pearson(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "    print(f\"Metrics: MSE={mse:.4f}, CI={ci:.4f}, R2={r2:.4f}, AUC-PR={auc_pr:.4f}, RM²={rm2:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "# Train and evaluate for all splits\n",
    "splits = {\n",
    "    \"No New Proteins in Test\": (\"Train_Data.csv\", \"Test_Data.csv\"),\n",
    "    \"New Proteins in Test\": (\"Train_Data_new_proteins.csv\", \"Test_Data_new_proteins.csv\"),\n",
    "    \"No New Drugs in Test\": (\"Train_Data_no_new_drugs.csv\", \"Test_Data_no_new_drugs.csv\"),\n",
    "    \"New Drugs in Test\": (\"Train_Data_new_drugs.csv\", \"Test_Data_new_drugs.csv\"),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Main loop for all splits\n",
    "for split_name, (train_path, test_path) in splits.items():\n",
    "    trained_model, train_loader = train_on_split(train_path, split_name)\n",
    "    evaluate_on_split(trained_model, test_path, split_name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
